{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d402d9",
   "metadata": {},
   "source": [
    "### 2.2 Question 2\n",
    "\n",
    "Download the Iris dataset from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/. The dataset can be downloaded from iris.data. Load the data into a pandas dataframe.\n",
    "For this lab, we’re going to be performing a binary classification problem,\n",
    "but this dataset has 3 classes: setosa, virginica, and versicolor. So we want\n",
    "to take this multi-class problem and transform it into a binary classification.\n",
    "Create a new column for the dataset called target. The value of target\n",
    "will be 1 if the row contains a setosa flower, else the value is $0$. There should\n",
    "be $\\frac{1}{3}$ rows with the value of $1$, the rest should be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f45ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal_Length</th>\n",
       "      <th>Sepal_Width</th>\n",
       "      <th>Petal_Length</th>\n",
       "      <th>Petal_Width</th>\n",
       "      <th>Species</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal_Length  Sepal_Width  Petal_Length  Petal_Width         Species  \\\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa   \n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa   \n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa   \n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa   \n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa   \n",
       "..            ...          ...           ...          ...             ...   \n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica   \n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica   \n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica   \n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica   \n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica   \n",
       "\n",
       "     target  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "..      ...  \n",
       "145       0  \n",
       "146       0  \n",
       "147       0  \n",
       "148       0  \n",
       "149       0  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Defining the fielpath\n",
    "filepath = \"data\\iris.data\" \n",
    "\n",
    "def load_data(filepath):\n",
    "    \n",
    "    # This part reads the data and avoids to use the first row as the column name\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "    \n",
    "    # After that the column names for the Dataset are assigned\n",
    "    col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Species']\n",
    "    df.columns = col_names  \n",
    "    \n",
    "    # Then the column \"tareget\" is created and filled with ceros\n",
    "    df['target']  = 0\n",
    "    \n",
    "    # Then the values in \"target\" column are modified depending if the Specie is or not a Setosa\n",
    "    df.loc[df['Species'] == 'Iris-setosa', 'target'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# The previous function is called\n",
    "iris  = load_data(filepath)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374e109",
   "metadata": {},
   "source": [
    "### 2.3 Question 3\n",
    "For this question we want to take this dataset of 150 rows, and split it into\n",
    "a train, test, and validation dataset, using the following proportions for each\n",
    "split:\n",
    "\n",
    "• Training: 70%\n",
    "\n",
    "• Validation: 10%\n",
    "\n",
    "• Testing: 20%\n",
    "\n",
    "Sample data for each subset using stratified sampling. I.e. the training data should have roughly $\\frac{1}{3}$\n",
    "positive samples, the testing and validation dataset should also have roughly $\\frac{1}{3}$ positive samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d597eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-setosa        50\n",
       "Iris-versicolor    50\n",
       "Iris-virginica     50\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a28480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slipt_data(df, column):\n",
    "    \n",
    "    values = list(df[column].unique())\n",
    "    \n",
    "    training_set = pd.DataFrame()\n",
    "    testing_set = pd.DataFrame()\n",
    "    validation_set = pd.DataFrame()\n",
    "    \n",
    "    for value in values:\n",
    "        \n",
    "        data = df[df[column] == value]\n",
    "        train=data.sample(n = 35,random_state=42)\n",
    "        test=data.drop(train.index)\n",
    "        validation = test.sample(n=5,random_state=42)\n",
    "        test=test.drop(validation.index)\n",
    "        \n",
    "        training_set = pd.concat([training_set, train])\n",
    "        testing_set = pd.concat([testing_set, test])\n",
    "        validation_set = pd.concat([validation_set, validation])\n",
    "\n",
    "    return training_set, testing_set, validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80e0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, testing_set, validation_set = slipt_data(iris, 'Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0601325",
   "metadata": {},
   "source": [
    "### 2.4 Question 4\n",
    "Using the linear regression model you created in the previous lecture, transform it into a logistic regressor by applying the logistic function to the output\n",
    "of the model. The loss function for this model should be binary cross entropy.\n",
    "\n",
    "Select two columns from the Iris dataset (i.e. petal length and petal\n",
    "width), and using these two columns, train a logistic regressor using gradient\n",
    "descent, measuring the gradient using finite differences approximation. This\n",
    "means that instead of having a single slope variable, we have multiple:\n",
    "\n",
    "$$\\hat{y} = \\sigma\\left (\\beta_{0} + \\sum_{i = 1}^{m}x_{i}\\beta_{i}\\right )$$\n",
    "\n",
    "\n",
    "where $\\hat{y}$ is the model’s probability prediction, $\\sigma$ is the logistic/sigmoid\n",
    "function, $\\beta_{0}$ is the intercept, $\\beta_{i}$\n",
    "is the coefficient that modulates the $X_{i}$ variable.\n",
    "\n",
    "I’ve made a start for you, please fill in the ’#TODOs’:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09978582",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = validation_set[[\"Petal_Length\", \"Petal_Width\"]]\n",
    "y = validation_set['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a194840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(x, weights):\n",
    "    \n",
    "    h =  weights[0] + np.dot(weights[1:], x.T)\n",
    "    \n",
    "    return h\n",
    "\n",
    "def sigmoid(h):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-h))\n",
    "\n",
    "def bce(y, z):\n",
    "    \n",
    "    negative_side = (1-y)* np.log(z)\n",
    "    positive_side = (y)* np.log(z)\n",
    "    N = len(y)\n",
    "    loss = (-1/N)* np.sum(positive_side + negative_side)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10b7dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y, yhat):\n",
    "    \n",
    "    # TODO: apply the binary cross entropy function returning the loss\n",
    "    negative_side = (1-y)* np.log(yhat)\n",
    "    positive_side = (y)* np.log(yhat)\n",
    "    N = len(y)\n",
    "    loss = (-1/N)* np.sum(positive_side + negative_side)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    def __init__(self, n_features: int = 2):\n",
    "\n",
    "        self.params = np.random.randn(n_features + 1)\n",
    "\n",
    "    def logistic(self, x):\n",
    "        # TODO: apply the logistic function\n",
    "\n",
    "        h = self.params[0] + np.dot(self.params[1:], x.T)\n",
    "        z = 1 / (1 + np.exp(-h))\n",
    "        return z\n",
    "\n",
    "    def __call__(self, x, logits=False):\n",
    "                                    \n",
    "        y = self.params[0] + self.params[1:] @ x.T\n",
    "\n",
    "        if not logits:\n",
    "            y = self.logistic(y)\n",
    "        return y\n",
    "\n",
    "    def fit(self, train_x, train_y, epochs: int = 100, lr: float = 0.01):\n",
    "        # TODO: train the model using gradient descent and finite-differences\n",
    "        delta_w = np.zeros(self.params.shape)\n",
    "        n = len(train_y)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(train_x.values, train_y.values):\n",
    "                # calculate loss and update model parameters using gradient descent\n",
    "                zi = self.logistic(xi)\n",
    "                error_term_i = zi - yi\n",
    "                delta_w[0]  +=  error_term_i\n",
    "                delta_w[1:] += np.dot(error_term_i, xi)\n",
    "        \n",
    "            self.params -= lr * delta_w / n\n",
    "            print(self.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "256c4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y, yhat):\n",
    "    \n",
    "    # TODO: apply the binary cross entropy function returning the loss\n",
    "    negative_side = (1-y)* np.log(yhat)\n",
    "    positive_side = (y)* np.log(yhat)\n",
    "    N = len(y)\n",
    "    loss = (-1/N)* np.sum(positive_side + negative_side)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    def __init__(self, n_features: int = 2):\n",
    "\n",
    "        self.params = np.random.randn(n_features + 1)\n",
    "\n",
    "    def logistic(self, x):\n",
    "        # TODO: apply the logistic function\n",
    "\n",
    "        h = self.params[0] + np.dot(self.params[1:], x.T)\n",
    "        z = 1 / (1 + np.exp(-h))\n",
    "        return z\n",
    "\n",
    "    def __call__(self, x, logits=False):\n",
    "                                    \n",
    "        y = self.params[0] + self.params[1:] @ x.T\n",
    "\n",
    "        if not logits:\n",
    "            y = self.logistic(y)\n",
    "        return y\n",
    "\n",
    "    def fit(self, train_x, train_y, epochs: int = 100, lr: float = 0.01):\n",
    "        # TODO: train the model using gradient descent and finite-differences\n",
    "        \n",
    "        delta_w = np.zeros(self.params.shape)\n",
    "        n = len(train_y)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(train_x.values, train_y.values):\n",
    "                # calculate loss and update model parameters using gradient descent\n",
    "                zi = self.logistic(xi)\n",
    "                error_term_i = zi - yi\n",
    "                delta_w[0]  +=  error_term_i\n",
    "                delta_w[1:] += np.dot(error_term_i, xi)\n",
    "                \n",
    "            self.params -= lr * delta_w / n\n",
    "            z = self.logistic(train_x.values)\n",
    "            loss = bce(train_y.values, z)\n",
    "            \n",
    "            for xi, yi in zip(valid_x, valid_y):\n",
    "                \n",
    "            # calculate validation loss (BUT DON'T UPDATE MODEL PARAMETERS!)\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aacb667b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54256004, -0.46341769, -0.46572975])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = LogisticRegressor()\n",
    "b.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd9c74f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54256004, -0.46341769, -0.46572975])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c6f976b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1184775352810608\n",
      "2.113948025623843\n",
      "2.107245210250629\n",
      "2.0984777127892897\n",
      "2.087788485046392\n",
      "2.0753531568389856\n",
      "2.061377930588103\n",
      "2.0460970015119626\n",
      "2.0297694894412173\n",
      "2.012675877869525\n",
      "1.99511396997127\n",
      "1.9773943896610366\n",
      "1.9598356775249488\n",
      "1.9427590552723586\n",
      "1.926482956402919\n",
      "1.911317442894737\n",
      "1.8975586456524918\n",
      "1.88548337820672\n",
      "1.8753440772778707\n",
      "1.867364219656587\n",
      "1.8617343526384629\n",
      "1.858608856021741\n",
      "1.8581035290162773\n",
      "1.8602940671213946\n",
      "1.8652154638013299\n",
      "1.8728623410005127\n",
      "1.883190182232619\n",
      "1.8961174129608949\n",
      "1.911528246055936\n",
      "1.929276186216759\n",
      "1.9491880675223718\n",
      "1.971068483995475\n",
      "1.9947044653266852\n",
      "2.019870249437377\n",
      "2.0463320104292686\n",
      "2.073852414028607\n",
      "2.1021948915657145\n",
      "2.1311275460514714\n",
      "2.1604266280398807\n",
      "2.1898795428017155\n",
      "2.2192873722880377\n",
      "2.248466914289799\n",
      "2.2772522564465305\n",
      "2.3054959141246094\n",
      "2.3330695688427143\n",
      "2.359864448294876\n",
      "2.3857913906814003\n",
      "2.410780635626104\n",
      "2.434781382034646\n",
      "2.4577611503644645\n"
     ]
    }
   ],
   "source": [
    "b.fit(x,y, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c003c896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49671415 -0.1382643   0.64768854]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 8\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m(lm(x, weights))\n\u001b[0;32m      9\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "weights = np.random.randn(3)\n",
    "epochs = 100\n",
    "\n",
    "print(weights)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    z = sigmoid(lm(x, weights))\n",
    "    N = len(y)\n",
    "    print(\"--------------\")\n",
    "    print(bce(y, z))\n",
    "    weights[0] -= 0.5 * np.sum(z-y) / N\n",
    "    weights[1:] -= 0.5 * np.dot ((z-y), x)/ N\n",
    "    print(weights)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c97e9",
   "metadata": {},
   "source": [
    "### 2.5 Question 5\n",
    "As gradient descent is iterating, store (using class variables), the training and validation loss.\n",
    "\n",
    "Visualise the training and validation loss. Is there a point at which the model begins to over fit? How do you know that the model is beginning to overfit by looking at these curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706eb33d",
   "metadata": {},
   "source": [
    "### 2.6 Question 6\n",
    "Predict the class labels for the testing set.\n",
    "For the testing set, calculate the:\n",
    "\n",
    "• TP – number of true positives\n",
    "\n",
    "• TN – number of true negatives\n",
    "\n",
    "• FP – number of false positives\n",
    "\n",
    "• FN – number of false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a43e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y, yhat):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the elements of a two-dimensional confusion matrix \n",
    "    calculated by counting the true values resulting of using logical operators such that:\n",
    "\n",
    "        TP = Logical AND\n",
    "        TN = Logical NOR\n",
    "        FP = Logical A'B\n",
    "        FN = Logical AB'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Logical AND Gate\n",
    "    TP = np.sum(np.logical_and(y, yhat))\n",
    "    \n",
    "    # Logical NOR Gate\n",
    "    TN = np.sum(np.logical_and(np.logical_not(y),np.logical_not(yhat)))\n",
    "    \n",
    "    # Logical A'B Gate\n",
    "    FP = np.sum(np.logical_and(np.logical_not(y),yhat))\n",
    "    \n",
    "    # Logical AB' Gate\n",
    "    FN = np.sum(np.logical_and(y,np.logical_not(yhat)))\n",
    "    \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe913794",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =    np.array([1,1,0,1,0,0])\n",
    "yhat = np.array([0,0,1,1,0,0])\n",
    "\n",
    "\n",
    "\n",
    "TP, TN, FP, FN = confusion_matrix(y, yhat)\n",
    "\n",
    "print(\"TP \"+str(TP) + \",TN \"+ str(TN) + \",FP \"+ str(FP) + \",FN \"+ str(FN) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08b1ac",
   "metadata": {},
   "source": [
    "### 2.7 Question 7\n",
    "Calculate the precision and recall and F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad65d9",
   "metadata": {},
   "source": [
    "$$Precission = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$F_{\\beta} = (1 + \\beta^{2})\\frac{Precission * Recall}{(\\beta^{2}* Precision) + Recall)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ea48a",
   "metadata": {},
   "source": [
    "Calling the above function and making use of the previous equations we have that the metrics of the confusion matrix are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision(y, yhat):\n",
    "    # calculate the precision and return it\n",
    "    TP, TN, FP, FN = confusion_matrix(y, yhat)  \n",
    "    pr = TP / (TP + FP)\n",
    "    \n",
    "    return pr\n",
    "\n",
    "def recall(y, yhat):\n",
    "    # calculate the recall and return it\n",
    "    TP, TN, FP, FN = confusion_matrix(y, yhat)\n",
    "    rc = TP / (TP + FN)\n",
    "    \n",
    "    return rc\n",
    "\n",
    "def f_beta(y, yhat, beta=1):\n",
    "    \n",
    "    pr = precision(y, yhat)\n",
    "    rc = recall(y, yhat)\n",
    "    \n",
    "    # calculate the f_beta score and return it\n",
    "    fb = (1 + beta**2) * ((pr * rc) / ((beta**2 * pr) + rc))\n",
    "    \n",
    "    return fb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e81b5",
   "metadata": {},
   "source": [
    "### 2.8 Question 8\n",
    "Generate a report using the precision, recall and F1 and confusion matrix.\n",
    "The report should be printed like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4ad0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = \"|\" + \" \" * 8 + \"|\" + \" \" * 10 + \"| Predicted |\" + \" \" *10 + \"|\\n\"\n",
    "t2 = \"|\" + \" \" * 8 + \"|\" + \" \" * 10 + \"|  Positive |\" + \" Negative |\\n\"\n",
    "t3 = \"| Actual |\" + \" Positive |\" + \" \" * (10 - len(str(TP))) + str(TP) + \" |\" + \" \" * (9 - len(str(FN))) + str(FN) + \" |\\n\"\n",
    "t4 = \"|\" + \" \" * 8 + \"|\" + \" Negative |\" + \" \" * (10 - len(str(FP))) + str(FP) + \" |\" + \" \" * (9 - len(str(TN))) + str(TN) + \" |\\n\"\n",
    "\n",
    "print(t1+t2+t3+t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db1c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb8ea0",
   "metadata": {},
   "source": [
    "### 2.9 Question 9\n",
    "Calculate the true-positive and false positive rate, and from these values\n",
    "generate a ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(y, yhat, threshold_step=0.01):\n",
    "# iteratively increase the threshold by threshold_step,\n",
    "# calculating the TP and FP rate for each iteration. This function\n",
    "# should return two lists, a list of TP rates, and a list of FP\n",
    "# rates.\n",
    "    return tp, fp\n",
    "\n",
    "tp, fp = roc(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e280b5",
   "metadata": {},
   "source": [
    "### 2.10 Question 10\n",
    "Now that you’ve created a logistic classifier for two features of the Iris dataset\n",
    "and have created some analytic results. Select another two columns (i.e.\n",
    "petal width and sepal length, or petal length and sepal width). Create\n",
    "a different logistic classifier using these new columns and create the same\n",
    "results as you did with questions 8 and 9.\n",
    "Compare these two models trained with different columns. Which model\n",
    "is best, and why do we know that it’s the best?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
